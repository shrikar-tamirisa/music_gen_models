Generative machine learning models have been extensively applied to image and prompt 
response generation. The field of music generation though is a relatively uncharted territory. 
This can be attributed to factors such as inherent temporal dependencies of music, 
subjectivity, multi-modal nature, lack of explicit rules, high dimensionality, limited training 
data and requirement of high domain knowledge. This project aims to explore machine 
learning models to generate musical notes, in particular piano renditions. We start with LSTM 
networks which enable us to capture temporal dependencies, and later move on to Generative 
Adversarial Networks to combine definiteness of neural networks with capabilities of LSTM. 
A thorough analysis of the results obtained is done to compare generated music with the 
training dataset, including rendering of pitch class and note length transition matrices. Further, 
techniques such as Wasserstein loss function, random noise injection and L2 kernel 
regularization are adopted to deal with mode collapse of GAN and diversify the generated 
samples. 
